{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pdfplumber scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26A8KI7WSmDb",
        "outputId": "0be0f1b6-3f59-466b-ab4d-39c031356009"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Step 2: Chunk the text\n",
        "def chunk_text(text, chunk_size=1000):\n",
        "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Step 3: Create TF-IDF Vectorizer\n",
        "def create_vectorizer(chunks):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "# Step 4: Initialize the language model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Step 5: Retrieve relevant chunks\n",
        "def retrieve(query, vectorizer, tfidf_matrix, top_n=3):\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    cosine_similarities = (query_vector * tfidf_matrix.T).toarray()\n",
        "    most_similar_indices = np.argsort(cosine_similarities[0])[-top_n:][::-1]\n",
        "    return most_similar_indices, cosine_similarities\n",
        "\n",
        "# Step 6: Generate concise and relevant response\n",
        "def generate_response(query, pdf_path):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    chunks = chunk_text(text)\n",
        "    vectorizer, tfidf_matrix = create_vectorizer(chunks)\n",
        "\n",
        "    indices, cosine_similarities = retrieve(query, vectorizer, tfidf_matrix)\n",
        "    contexts = [chunks[i] for i in indices]\n",
        "\n",
        "    # Combine contexts for better relevance\n",
        "    combined_context = \" \".join(contexts)\n",
        "\n",
        "    # Check if the combined context is relevant\n",
        "    if len(combined_context) < 50 or all(cosine_similarities[0][i] < 0.1 for i in indices):  # Arbitrary threshold for context length and relevance\n",
        "        return \"I do not know.\"\n",
        "\n",
        "    # Generate a concise answer based on the relevant context\n",
        "    response = generator(\n",
        "        f\"Please provide a concise answer to the question: '{query}' based on the following context: {combined_context}\",\n",
        "        max_length=50,  # Set this to a value greater than the input length\n",
        "        max_new_tokens=20,  # Specify how many new tokens to generate\n",
        "        truncation=True\n",
        "    )\n",
        "    generated_text = response[0]['generated_text'].strip()\n",
        "\n",
        "    # Clean up the generated text to make it more readable\n",
        "    if \"Answer the question:\" in generated_text:\n",
        "        generated_text = generated_text.split(\"Answer the question:\")[1].strip()\n",
        "\n",
        "    return generated_text if generated_text else \"I do not know.\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"MLMaterial.pdf\"  # Ensure to provide a valid PDF file path\n",
        "    user_query = input(\"Please enter your query: \")  # Allow user to input their query\n",
        "    response = generate_response(user_query, pdf_path)\n",
        "\n",
        "    # Display only the query and the response\n",
        "    print(response)  # Only print the response without any additional context or information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD5-GxvOS6JZ",
        "outputId": "04abf27b-a144-4924-851d-10ddf86c7060"
      },
      "execution_count": 74,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your query: what is overfitting?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Both `max_new_tokens` (=20) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide a concise answer to the question: 'what is overfitting?' based on the following context: of overfitting increase as much we provide training to our\n",
            "▪ model.\n",
            "▪ It means the more we train our model, the more chances of occurring the overfitted\n",
            "▪ model.\n",
            "▪ Overfitting is the main problem that occurs in supervised learning.\n",
            "▪ Avoid the Overfitting in Model\n",
            "▪ Cross-Validation\n",
            "▪ Training with more data\n",
            "▪ Removing features\n",
            "▪ Early stopping the training\n",
            "▪ Regularization\n",
            "▪ Ensembling\n",
            "• Goodness of Fit\n",
            "▪ The \"Goodness of fit\" term is taken from the statistics, and the goal of the machine learning\n",
            "models to achieve the goodness of fit.\n",
            "▪ In statistics modelling, it defines how closely the result or predicted values match the true\n",
            "values of the dataset.\n",
            "▪ The model with a good fit is between the underfitted and overfitted model, and ideally, it\n",
            "makes predictions with 0 errors, but in practice, it is difficult to achieve it.\n",
            "▪ As when we train our model for a time, the errors in the training data go down, and the same\n",
            "happens with test data.\n",
            "▪ But if we train the model for a long durati oudy, ?, Normal, Strong, Warm, Same >\n",
            "G3: <?, ?, ?, ?, ?, ?>\n",
            "Note: Which is not accepting for the Negative classification.\n",
            "Significance:\n",
            "Understanding and managing inductive bias is crucial in machine learning to strike a balance\n",
            "between underfitting and overfitting. It involves making intentional choices about the model's\n",
            "assumptions to achieve optimal performance on both training and unseen data.\n",
            "Generalization\n",
            "• Definition: How well a model trained on the training set predicts the right output for new\n",
            "instances.\n",
            "• For best generalization, we should match the complexity of the hypothesis class H with the\n",
            "complexity of the function underlying the data.\n",
            "• Under-Fitting: Occurs when a model is too simple and fails to capture the underlying patterns\n",
            "in the data. High bias, low variance.\n",
            "▪ Underfitting occurs when our machine learning model is not able to capture the underlying\n",
            "trend of the data.\n",
            "▪ To avoid the overfitting in the model, the fed of training data can be stopped at an\n",
            "▪ earl istent with d, and some member of G is more general than h\n",
            "iii. Remove from S any hypothesis that is more general than another hypothesis in S\n",
            "5. If d is a negative example\n",
            "a. Remove from S any hypothesis inconsistent with d\n",
            "b. For each hypothesis g in G that is not consistent with d\n",
            "i. Remove g from G\n",
            "ii. Add to G all minimal specializations h of g such that\n",
            "▪ h is consistent with d, and some member of S is more specific than h\n",
            "iii. Remove from G any hypothesis that is less general than another hypothesis in G\n",
            "Advantages:\n",
            "▪ Expressiveness: More expressive than Find-S as it maintains a version space\n",
            "▪ Adaptability: Adapt to new examples by updating the version space\n",
            "Disadvantages:\n",
            "▪ Computational Complexity: complexity may increase with the size of the hypothesis space and the\n",
            "number of training examples, making it computationally expensive in certain cases.\n",
            "▪ Sensitivity to Noise\n",
            "▪ Handling Continuous Attributes\n",
            "▪ Overfitting: if the training data contains noise or if the hypothesis s less we need to take train more, and have to follow up on our training. To summarize\n"
          ]
        }
      ]
    }
  ]
}